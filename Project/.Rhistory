rm(list=ls())
library(gbm)
data <- read.csv("Train_Digits_20171108.csv", header=TRUE)
data <- read.csv("Train_Digits_20171108.csv")
data$Digit <- data$Digit%%2
data$Digit <- as.factor(data$Digit)
sample <- sample(nrow(data)*0.8)
train <- data[sample,]
test <- data[-sample,]
ntrees = 10
cvfolds = 2
intdepth = 2
boost <- gbm(train$Digit ~., data=train, distribution="multinomial", n.trees=ntrees, interaction.depth=intdepth, shrinkage=0.001, bag.fraction=1, cv.folds=cvfolds, n.cores=4)
warnings()
rm(list=ls())
library(gbm)
library(caret)
data <- read.csv("Train_Digits_20171108.csv", header=TRUE)
data <- read.csv("Train_Digits_20171108.csv")
data$Digit <- data$Digit%%2
data$Digit <- as.factor(data$Digit)
sample <- sample(nrow(data)*0.8)
train <- data[sample,]
test <- data[-sample,]
nzr <- nearZeroVar(train[,-1],saveMetrics=T,freqCut=10000/1,uniqueCut=1/7)
cutvar <- rownames(nzr[nzr$nzv==TRUE,])
var <- setdiff(names(train),cutvar)
train <- train[,var]
ntrees = 10
cvfolds = 2
intdepth = 2
boost <- gbm(train$Digit ~., data=train, distribution="multinomial", n.trees=ntrees, interaction.depth=intdepth, shrinkage=0.001, bag.fraction=1, cv.folds=cvfolds, n.cores=4)
print("Here")
mean(boost$cv.error)
plot(summary(boost))
write.csv(boost$cv.error, file=paste(paste("exports/boost_error", Sys.time(), sep=""), ".csv", sep=""))
plotName <- paste(paste(paste(paste(paste(paste('ntrees=',ntrees,sep=""), '_cvfolds=',sep=""), cvfolds,sep=""),'_depth=', sep=""), intdepth, sep=""),".pdf",sep="")
pdf(paste("exports/",plotName,sep=""))
plot(boost$cv.error,type="l", ylab = "CV error", xlab = "Number of trees", main = "Boosted Model")
mean(boost$cv.error)
varImpPlot(boost)
plot(boost$cv.error, type="l", ylab="CV error", xlab="Number of trees", main="Boosted Model")
label_train <- train$Digit
train$Digit <- NULL
pred_train <- predict(rf, newdata=train)
label_test <- test$Digit
test$Digit <- NULL
pred_test <- predict(boost, newdata=test)
confusionMatrix(pred_test, label_test)
confusionMatrix(pred_test, label_test)
label_test <- test$Digit
test <- test[,var[-1]]
pred_test <- predict(boost, newdata=test)
confusionMatrix(pred_test, label_test)
rm(list=ls())
library(gbm)
library(caret)
data <- read.csv("Train_Digits_20171108.csv", header=TRUE)
data <- read.csv("Train_Digits_20171108.csv")
data$Digit <- data$Digit%%2
data$Digit <- as.factor(data$Digit)
sample <- sample(nrow(data)*0.8)
train <- data[sample,]
test <- data[-sample,]
nzr <- nearZeroVar(train[,-1],saveMetrics=T,freqCut=10000/1,uniqueCut=1/7)
cutvar <- rownames(nzr[nzr$nzv==TRUE,])
var <- setdiff(names(train),cutvar)
train <- train[,var]
ntrees = 100
cvfolds = 2
intdepth = 2
boost <- gbm(train$Digit ~., data=train, distribution="multinomial", n.trees=ntrees, interaction.depth=intdepth, shrinkage=0.001, bag.fraction=1, cv.folds=cvfolds, n.cores=4)
mean(boost$cv.error)
varImpPlot(boost)
plot(boost$cv.error, type="l", ylab="CV error", xlab="Number of trees", main="Boosted Model")
label_train <- train$Digit
train$Digit <- NULL
pred_train <- predict(boost, newdata=train)
label_test <- test$Digit
test <- test[,var[-1]]
pred_test <- predict(boost, newdata=test)
confusionMatrix(pred_test, label_test)
length(pred_test)
length(label_test)
length(test)
test <- test[,var[-1]]
label_test <- test$Digit
pred_test <- predict(boost, newdata=test)
confusionMatrix(pred_test, label_test)
rm(list=ls())
library(gbm)
library(caret)
data <- read.csv("Train_Digits_20171108.csv", header=TRUE)
data <- read.csv("Train_Digits_20171108.csv")
data$Digit <- data$Digit%%2
data$Digit <- as.factor(data$Digit)
sample <- sample(nrow(data)*0.8)
train <- data[sample,]
test <- data[-sample,]
nzr <- nearZeroVar(train[,-1],saveMetrics=T,freqCut=10000/1,uniqueCut=1/7)
cutvar <- rownames(nzr[nzr$nzv==TRUE,])
var <- setdiff(names(train),cutvar)
train <- train[,var]
ntrees = 100
cvfolds = 2
intdepth = 2
boost <- gbm(train$Digit ~., data=train, distribution="multinomial", n.trees=ntrees, interaction.depth=intdepth, shrinkage=0.001, bag.fraction=1, cv.folds=cvfolds, n.cores=4)
mean(boost$cv.error)
plot(boost$cv.error, type="l", ylab="CV error", xlab="Number of trees", main="Boosted Model")
label_train <- train$Digit
train$Digit <- NULL
pred_train <- predict(boost, newdata=train)
test <- test[,var[-1]]
label_test <- test$Digit
pred_test <- predict(boost, newdata=test)
confusionMatrix(pred_test, label_test)
length(label_test)
test <- data[-sample,]
label_test <- test$Digit
length(label_test)
length(test)
length(test$Digit)
test <- test[,var[-1]]
length(test$Digit)
length(test[,1])
pred_test <- predict(boost, newdata=test)
confusionMatrix(pred_test, label_test)
pred_test <- predict(boost, newdata=test, type="response")
confusionMatrix(pred_test, label_test)
print(pred_test)
predict(boost, newdata=test, type="response")$0
pred_test$0
predict(boost, newdata=test, type="response")[,1]
pred_test[,1]
pred_test
prediction <- data.frame(ImageId=1:nrow(test),Label=pred_test)
prediction
prediction[,1]
prediction[,2]
prediction[,3]
prediction$est <- prediction[,2] < 0.5
prediction
prediction$est <- as.numeric(prediction[,2] < 0.5)
prediction
confusionMatrix(prediction$est, label_test)
rm(list=ls())
rm(list=ls())
rm(list=ls())
library(gbm)
library(caret)
data <- read.csv("Train_Digits_20171108.csv", header=TRUE)
data <- read.csv("Train_Digits_20171108.csv")
data$Digit <- data$Digit%%2
data$Digit <- as.factor(data$Digit)
sample <- sample(nrow(data)*0.8)
train <- data[sample,]
test <- data[-sample,]
nzr <- nearZeroVar(train[,-1],saveMetrics=T,freqCut=10000/1,uniqueCut=1/7)
cutvar <- rownames(nzr[nzr$nzv==TRUE,])
var <- setdiff(names(train),cutvar)
train <- train[,var]
ntrees = 2000
cvfolds = 2
intdepth = 2
boost <- gbm(train$Digit ~., data=train, distribution="multinomial", n.trees=ntrees, interaction.depth=intdepth, shrinkage=0.001, bag.fraction=1, cv.folds=cvfolds, n.cores=4)
mean(boost$cv.error)
plot(boost$cv.error, type="l", ylab="CV error", xlab="Number of trees", main="Boosted Model")
label_train <- train$Digit
train$Digit <- NULL
pred_train <- predict(boost, newdata=train)
label_test <- test$Digit
test <- test[,var[-1]]
pred_test <- predict(boost, newdata=test, type="response")
prediction <- data.frame(ImageId=1:nrow(test),Label=pred_test)
prediction$est <- as.numeric(prediction[,2] < 0.5)
confusionMatrix(prediction$est, label_test)
rm(list=ls())
library(randomForest)
library(doParallel)
library(caret)
ntrees = 100
cvfolds = 10
intdepth = 2
set.seed(100)
data <- read.csv("Train_Digits_20171108.csv", header=TRUE)
data$Digit <- data$Digit%%2
data$Digit <- as.factor(data$Digit)
sample <- sample(nrow(data)*0.8)
train <- data[sample,]
test <- data[-sample,]
nzr <- nearZeroVar(train[,-1],saveMetrics=T,freqCut=10000/1,uniqueCut=1/7)
cutvar <- rownames(nzr[nzr$nzv==TRUE,])
var <- setdiff(names(train),cutvar)
train <- train[,var]
rf <- randomForest(Digit ~ ., data=train, ntree=ntrees, importance=TRUE, na.action = na.exclude)
varImpPlot(rf, type=2, main="Random Forest Variable Importance Plot")
err <- data.frame(x = c(1:ntrees), error = rf$err.rate)
plot(rf$err.rate[,"OOB"],type="l", xlab="Number of trees", ylab="OOB error")
plotName <- paste(paste(paste(paste(paste(paste('ntrees=',ntrees,sep=""), '_mtry=',sep=""), mtry,sep=""),'', sep=""), "s", sep=""),".pdf",sep="")
pdf(paste("exports/",plotName,sep=""))
plot(rf$"mse", type="l", xlab="Number of trees", ylab="MSE Error")
legend("topright", legend=c("Random Forest"), col=c("black"), lwd=2)
label_train <- train$Digit
train$Digit <- NULL
pred_train <- predict(rf, newdata=train)
label_test <- test$Digit
test <- test[,var[-1]]
pred_test <- predict(rf, newdata=test)
confusionMatrix(pred_test, label_test)
rm(list=ls())
library(randomForest)
library(doParallel)
library(caret)
ntrees = 100
cvfolds = 10
intdepth = 2
set.seed(100)
data <- read.csv("Train_Digits_20171108.csv", header=TRUE)
data$Digit <- data$Digit%%2
data$Digit <- as.factor(data$Digit)
sample <- sample(nrow(data)*0.8)
train <- data[sample,]
test <- data[-sample,]
rf <- randomForest(Digit ~ ., data=train, ntree=ntrees, importance=TRUE, na.action = na.exclude)
varImpPlot(rf, type=2, main="Random Forest Variable Importance Plot")
err <- data.frame(x = c(1:ntrees), error = rf$err.rate)
plot(rf$err.rate[,"OOB"],type="l", xlab="Number of trees", ylab="OOB error")
label_train <- train$Digit
train$Digit <- NULL
pred_train <- predict(rf, newdata=train)
label_test <- test$Digit
test$Digit <- NULL
pred_test <- predict(rf, newdata=test)
confusionMatrix(pred_test, label_test)
rm(list=ls())
library(randomForest)
library(doParallel)
library(caret)
ntrees = 100
cvfolds = 10
intdepth = 2
set.seed(100)
data <- read.csv("Train_Digits_20171108.csv", header=TRUE)
data$Digit <- data$Digit%%2
data$Digit <- as.factor(data$Digit)
sample <- sample(nrow(data)*0.8)
train <- data[sample,]
test <- data[-sample,]
nzr <- nearZeroVar(train[,-1],saveMetrics=T,freqCut=10000/1,uniqueCut=1/7)
cutvar <- rownames(nzr[nzr$nzv==TRUE,])
var <- setdiff(names(train),cutvar)
train <- train[,var]
rf <- randomForest(Digit ~ ., data=train, ntree=ntrees, importance=TRUE, na.action = na.exclude)
varImpPlot(rf, type=2, main="Random Forest Variable Importance Plot")
err <- data.frame(x = c(1:ntrees), error = rf$err.rate)
plot(rf$err.rate[,"OOB"],type="l", xlab="Number of trees", ylab="OOB error")
label_train <- train$Digit
train$Digit <- NULL
pred_train <- predict(rf, newdata=train)
label_test <- test$Digit
test <- test[,var[-1]]
pred_test <- predict(rf, newdata=test)
confusionMatrix(pred_test, label_test)
rm(list=ls())
library(h2o)
set.seed(111)
localH2O = h2o.init(nthreads=-1,            ## -1: use all available threads
max_mem_size = "2G")
h2o.removeAll()
data <- read.csv("Train_Digits_20171108.csv")
data$Digit <- data$Digit%%2
data$Digit <- as.factor(data$Digit)
sample <- sample(nrow(data)*0.8)
train <- data[sample,]
test <- data[-sample,]
nzr <- nearZeroVar(train[,-1],saveMetrics=T,freqCut=10000/1,uniqueCut=1/7)
cutvar <- rownames(nzr[nzr$nzv==TRUE,])
var <- setdiff(names(train),cutvar)
train <- train[,var]
test <- test[,var]
df_train <- as.h2o(train)
df_test <- as.h2o(test)
hyper_params <- list(
hidden = list(c(512, 128)),
epochs = c(10),
rate = c(0.005, 0.01),
stopping_rounds = c(3),
stopping_metric = "misclassification", # could be "MSE","logloss","r2"
stopping_tolerance = c(0.01, 0.02)
)
grid <- h2o.grid(
algorithm="deeplearning",
x=2:length(df_train[1,]),
y=1,
training_frame=df_train,
nfolds=5,
seed=111,
hyper_params=hyper_params
)
grid <- h2o.getGrid(grid@grid_id, sort_by="mse", decreasing=F)
summary(grid)
best_model <- h2o.getModel(grid@model_ids[[1]])
best_model
print(h2o.performance(best_model, newdata = df_test))
plot(grid@summary_table$mse)
rm(list=ls())
library(h2o)
set.seed(111)
localH2O = h2o.init(nthreads=-1,            ## -1: use all available threads
max_mem_size = "2G")
h2o.removeAll()
data <- read.csv("Train_Digits_20171108.csv")
data$Digit <- data$Digit%%2
data$Digit <- as.factor(data$Digit)
sample <- sample(nrow(data)*0.8)
train <- data[sample,]
test <- data[-sample,]
nzr <- nearZeroVar(train[,-1],saveMetrics=T,freqCut=10000/1,uniqueCut=1/7)
cutvar <- rownames(nzr[nzr$nzv==TRUE,])
var <- setdiff(names(train),cutvar)
train <- train[,var]
test <- test[,var]
df_train <- as.h2o(train)
df_test <- as.h2o(test)
hyper_params <- list(
hidden = list(c(512, 128), c(512, 128, 42)),
epochs = c(10),
rate = c(0.005),
stopping_rounds = c(3),
stopping_metric = "misclassification", # could be "MSE","logloss","r2"
stopping_tolerance = c(0.01, 0.02)
)
grid <- h2o.grid(
algorithm="deeplearning",
x=2:length(df_train[1,]),
y=1,
training_frame=df_train,
nfolds=5,
seed=111,
hyper_params=hyper_params
)
grid <- h2o.getGrid(grid@grid_id, sort_by="mse", decreasing=F)
summary(grid)
best_model <- h2o.getModel(grid@model_ids[[1]])
best_model
print(h2o.performance(best_model, newdata = df_test))
plot(grid@summary_table$mse)
